const GLOBAL_STOP_WORDS_ENGLSIH_SET = [
  "about",
  "after",
  "all",
  "also",
  "am",
  "an",
  "and",
  "another",
  "any",
  "are",
  "as",
  "at",
  "be",
  "because",
  "been",
  "before",
  "being",
  "between",
  "both",
  "but",
  "by",
  "came",
  "can",
  "come",
  "could",
  "did",
  "do",
  "each",
  "for",
  "from",
  "get",
  "got",
  "has",
  "had",
  "he",
  "have",
  "her",
  "here",
  "him",
  "himself",
  "his",
  "how",
  "if",
  "in",
  "into",
  "is",
  "it",
  "like",
  "make",
  "many",
  "me",
  "might",
  "more",
  "most",
  "much",
  "must",
  "my",
  "never",
  "now",
  "of",
  "on",
  "only",
  "or",
  "other",
  "our",
  "out",
  "over",
  "said",
  "same",
  "should",
  "since",
  "some",
  "still",
  "such",
  "take",
  "than",
  "that",
  "the",
  "their",
  "them",
  "then",
  "there",
  "these",
  "they",
  "this",
  "those",
  "through",
  "to",
  "too",
  "under",
  "up",
  "very",
  "was",
  "way",
  "we",
  "well",
  "were",
  "what",
  "where",
  "which",
  "while",
  "who",
  "with",
  "would",
  "you",
  "your",
  "a",
  "i",
];
// Enhanced stop words list
const STOP_WORDS = new Set([
  "a",
  "an",
  "and",
  "are",
  "as",
  "at",
  "be",
  "by",
  "for",
  "from",
  "has",
  "he",
  "in",
  "is",
  "it",
  "its",
  "of",
  "on",
  "that",
  "the",
  "to",
  "was",
  "will",
  "with",
  "about",
  "after",
  "all",
  "also",
  "am",
  "another",
  "any",
  "because",
  "been",
  "before",
  "being",
  "between",
  "both",
  "but",
  "came",
  "can",
  "come",
  "could",
  "did",
  "do",
  "each",
  "get",
  "got",
  "had",
  "have",
  "her",
  "here",
  "him",
  "himself",
  "his",
  "how",
  "if",
  "into",
  "like",
  "make",
  "many",
  "me",
  "might",
  "more",
  "most",
  "much",
  "must",
  "my",
  "never",
  "now",
  "only",
  "or",
  "other",
  "our",
  "out",
  "over",
  "said",
  "same",
  "should",
  "since",
  "some",
  "still",
  "such",
  "take",
  "than",
  "their",
  "them",
  "then",
  "there",
  "these",
  "they",
  "this",
  "those",
  "through",
  "too",
  "under",
  "up",
  "very",
  "way",
  "we",
  "well",
  "were",
  "what",
  "where",
  "which",
  "while",
  "who",
  "would",
  "you",
  "your",
  "i",
]);

// Domain-specific keyword boosts
const DOMAIN_KEYWORDS = {
  algorithms: {
    keywords: [
      "algorithm",
      "sort",
      "search",
      "complexity",
      "time",
      "space",
      "recursive",
      "divide",
      "conquer",
      "optimization",
      "efficient",
      "performance",
      "logarithmic",
      "linear",
      "quadratic",
    ],
    boost: 3.0,
  },
  "data structures": {
    keywords: [
      "structure",
      "node",
      "pointer",
      "array",
      "list",
      "tree",
      "graph",
      "hash",
      "stack",
      "queue",
      "heap",
      "linked",
      "binary",
      "table",
    ],
    boost: 3.0,
  },
  "operating systems": {
    keywords: [
      "system",
      "process",
      "thread",
      "memory",
      "page",
      "swap",
      "lock",
      "mutex",
      "semaphore",
      "scheduler",
      "kernel",
      "virtual",
      "physical",
      "cpu",
    ],
    boost: 3.0,
  },
  "cell biology": {
    keywords: [
      "cell",
      "membrane",
      "mitosis",
      "chromosome",
      "protein",
      "enzyme",
      "organelle",
      "nucleus",
      "cytoplasm",
      "dna",
      "rna",
      "cellular",
      "molecular",
      "biological",
    ],
    boost: 3.0,
  },
  "cs 101": {
    keywords: [
      "computer",
      "architecture",
      "binary",
      "notation",
      "programming",
      "cpu",
      "memory",
      "instruction",
      "bit",
      "byte",
      "computational",
      "digital",
      "von",
      "neumann",
    ],
    boost: 3.0,
  },
};
const StopWordsEngMap = new Map(
  GLOBAL_STOP_WORDS_ENGLSIH_SET.map((word) => [word, true])
);

function tokenizer(text) {
  return text
    .toLowerCase()
    .split(/[^a-z0-9]+/)
    .filter(Boolean);
}

function removeStopWords(tokens) {
  return tokens.filter((token) => !StopWordsEngMap.has(token));
}

function tf(text) {
  const tokens = removeStopWords(tokenizer(text));
  const freq = new Map();

  for (const token of tokens) {
    freq.set(token, (freq.get(token) || 0) + 1);
  }
  return freq;
}

module.exports = { STOP_WORDS, DOMAIN_KEYWORDS, tf };
